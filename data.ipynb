{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cek jumlah beserta struktur datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China_Drone -> Train Images: 2401, Test Images: 0, Annotations: 2401\n",
      "China_MotorBike -> Train Images: 1977, Test Images: 500, Annotations: 1977\n",
      "Czech -> Train Images: 2829, Test Images: 709, Annotations: 2829\n",
      "India -> Train Images: 7706, Test Images: 1959, Annotations: 7706\n",
      "Japan -> Train Images: 10506, Test Images: 2627, Annotations: 10506\n",
      "Norway -> Train Images: 8161, Test Images: 2040, Annotations: 8161\n",
      "United_States -> Train Images: 4805, Test Images: 1200, Annotations: 4805\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_path = \"D:\\Pothole Vision - AI Road Damage Detection\\dataset\\RDD2022_all_countries\"\n",
    "def count_files(path):\n",
    "    return len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]) if os.path.exists(path) else 0\n",
    "\n",
    "summary = []\n",
    "\n",
    "for country in os.listdir(root_path):\n",
    "    country_path = os.path.join(root_path, country)\n",
    "    if os.path.isdir(country_path):\n",
    "        train_images = count_files(os.path.join(country_path, \"train\", \"images\"))\n",
    "        test_images = count_files(os.path.join(country_path, \"test\", \"images\"))\n",
    "        xml_files = count_files(os.path.join(country_path, \"train\", \"annotations\", \"xmls\"))\n",
    "        summary.append(f\"{country} -> Train Images: {train_images}, Test Images: {test_images}, Annotations: {xml_files}\")\n",
    "\n",
    "for line in summary:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "China_drone tidak punya folder test, abaikan?\n",
    "Selanjutnya Saya akan split datset dari Train & Test menjadi Train, Test, & Val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika dataset DIGABUNG semua negara menjadi satu dataset besar:\n",
    "Keuntungan:\n",
    "\n",
    "Model akan lebih general karena belajar dari berbagai jenis jalan, cuaca, kamera.\n",
    "\n",
    "Bisa membantu jika nanti digunakan di Indonesia yang belum punya data.\n",
    "\n",
    "Jumlah data menjadi sangat besar (10.000++), sangat bagus untuk deep learning.\n",
    "\n",
    "Kekurangan:\n",
    "\n",
    "Bisa menyebabkan bias ke negara dengan data terbanyak (misalnya India, Japan).\n",
    "\n",
    "Anotasi antar negara mungkin memiliki inkonsistensi kecil (labeling style, noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rekomendasi untuk kasus ini:\n",
    "Karena kamu akan pakai untuk Indonesia, tapi belum punya data lokal, maka:\n",
    "\n",
    "Gabungkan semua negara → latih model global, supaya kuat terhadap variasi.\n",
    "\n",
    "Simpan metadata negara asalnya → bisa dipakai untuk evaluasi per negara.\n",
    "\n",
    "Nanti, jika ada data Indonesia, kamu bisa fine-tune model global ke data lokal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi label di semua dataset mentah:\n",
      "D10: 11830\n",
      "D00: 26016\n",
      "D20: 10617\n",
      "Repair: 1046\n",
      "D40: 6544\n",
      "Block crack: 3\n",
      "D44: 5057\n",
      "D01: 179\n",
      "D11: 45\n",
      "D50: 3581\n",
      "D43: 793\n",
      "D0w0: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "root_path = \"dataset/RDD2022_all_countries\"\n",
    "\n",
    "country_folders = [\n",
    "    \"China_Drone\", \"China_MotorBike\", \"Czech\",\n",
    "    \"India\", \"Japan\", \"Norway\", \"United_States\"\n",
    "]\n",
    "\n",
    "all_labels = []\n",
    "\n",
    "for country in country_folders:\n",
    "    annotation_folder = os.path.join(root_path, country, \"train\", \"annotations\", \"xmls\")\n",
    "    if os.path.exists(annotation_folder):\n",
    "        for filename in os.listdir(annotation_folder):\n",
    "            if filename.endswith('.xml'):\n",
    "                file_path = os.path.join(annotation_folder, filename)\n",
    "                try:\n",
    "                    tree = ET.parse(file_path)\n",
    "                    root = tree.getroot()\n",
    "                    for obj in root.findall('object'):\n",
    "                        label = obj.find('name').text.strip()\n",
    "                        all_labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error parsing {file_path}: {e}\")\n",
    "\n",
    "label_counts = Counter(all_labels)\n",
    "\n",
    "print(\"Distribusi label di semua dataset mentah:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses selesai.\n",
      "File XML dan gambar yang dihapus: 24837\n",
      "File XML yang dipertahankan: 13548\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "root_path = \"dataset/RDD2022_all_countries\"\n",
    "\n",
    "def clean_annotations(root_path, target_label='D00'):\n",
    "    removed_files = 0\n",
    "    kept_files = 0\n",
    "    for country in os.listdir(root_path):\n",
    "        country_path = os.path.join(root_path, country)\n",
    "        if not os.path.isdir(country_path):\n",
    "            continue\n",
    "\n",
    "        # Proses folder train annotations (ubah jika perlu val/test juga)\n",
    "        annotations_dir = os.path.join(country_path, 'train', 'annotations', 'xmls')\n",
    "        images_dir = os.path.join(country_path, 'train', 'images')\n",
    "\n",
    "        if not os.path.exists(annotations_dir):\n",
    "            print(f\"Folder anotasi tidak ditemukan: {annotations_dir}, dilewati.\")\n",
    "            continue\n",
    "\n",
    "        for xml_file in os.listdir(annotations_dir):\n",
    "            if not xml_file.endswith('.xml'):\n",
    "                continue\n",
    "\n",
    "            xml_path = os.path.join(annotations_dir, xml_file)\n",
    "            image_file = xml_file.replace('.xml', '.jpg')\n",
    "            image_path = os.path.join(images_dir, image_file)\n",
    "\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Cari objek yang labelnya bukan target_label, hapus mereka\n",
    "            objects = root.findall('object')\n",
    "            removed_objs = 0\n",
    "            for obj in objects:\n",
    "                label = obj.find('name').text.strip()\n",
    "                if label != target_label:\n",
    "                    root.remove(obj)\n",
    "                    removed_objs += 1\n",
    "\n",
    "            # Cek apakah setelah penghapusan masih ada objek\n",
    "            if len(root.findall('object')) == 0:\n",
    "                # Hapus XML dan gambarnya\n",
    "                os.remove(xml_path)\n",
    "                if os.path.exists(image_path):\n",
    "                    os.remove(image_path)\n",
    "                removed_files += 1\n",
    "            else:\n",
    "                # Simpan ulang XML yang sudah dibersihkan\n",
    "                tree.write(xml_path)\n",
    "                kept_files += 1\n",
    "\n",
    "    print(f\"Proses selesai.\")\n",
    "    print(f\"File XML dan gambar yang dihapus: {removed_files}\")\n",
    "    print(f\"File XML yang dipertahankan: {kept_files}\")\n",
    "\n",
    "clean_annotations(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset berhasil digabung dan displit.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Lokasi dataset dan output\n",
    "root_path = \"D:\\\\Pothole Vision - AI Road Damage Detection\\\\dataset\\\\RDD2022_all_countries\"\n",
    "output_path = \"D:\\\\Pothole Vision - AI Road Damage Detection\\\\dataset-mix\"\n",
    "train_val_split = 0.8  # 80% untuk train, 20% untuk val\n",
    "\n",
    "# Membuat direktori output\n",
    "os.makedirs(os.path.join(output_path, 'train', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, 'train', 'annotations'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, 'val', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, 'val', 'annotations'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, 'test', 'images'), exist_ok=True)\n",
    "\n",
    "# Metadata untuk csv\n",
    "metadata = []\n",
    "\n",
    "def copy_file(src_file, dest_file):\n",
    "    shutil.copy(src_file, dest_file)\n",
    "\n",
    "# Proses per negara\n",
    "for country in os.listdir(root_path):\n",
    "    country_path = os.path.join(root_path, country)\n",
    "    if not os.path.isdir(country_path):\n",
    "        continue\n",
    "\n",
    "    images_path = os.path.join(country_path, 'train', 'images')\n",
    "    annotations_path = os.path.join(country_path, 'train', 'annotations', 'xmls')\n",
    "\n",
    "    if not os.path.exists(images_path) or not os.path.exists(annotations_path):\n",
    "        continue\n",
    "\n",
    "    image_files = sorted([f for f in os.listdir(images_path) if f.endswith('.jpg')])\n",
    "    annotation_files = sorted([f for f in os.listdir(annotations_path) if f.endswith('.xml')])\n",
    "\n",
    "    # Pastikan hanya file yang cocok (image dan XML) yang digunakan\n",
    "    matched_files = []\n",
    "    for image in image_files:\n",
    "        basename = os.path.splitext(image)[0]\n",
    "        if f\"{basename}.xml\" in annotation_files:\n",
    "            matched_files.append((image, f\"{basename}.xml\"))\n",
    "\n",
    "    for image, annotation in matched_files:\n",
    "        src_image = os.path.join(images_path, image)\n",
    "        src_annotation = os.path.join(annotations_path, annotation)\n",
    "\n",
    "        split = 'train' if random.random() < train_val_split else 'val'\n",
    "        dest_image = os.path.join(output_path, split, 'images', image)\n",
    "        dest_annotation = os.path.join(output_path, split, 'annotations', annotation)\n",
    "\n",
    "        copy_file(src_image, dest_image)\n",
    "        copy_file(src_annotation, dest_annotation)\n",
    "\n",
    "        metadata.append({'filename': image, 'country': country, 'split': split})\n",
    "\n",
    "    # Test set (copy ke test/images tanpa anotasi)\n",
    "    test_images_path = os.path.join(country_path, 'test', 'images')\n",
    "    if os.path.exists(test_images_path):\n",
    "        for test_img in os.listdir(test_images_path):\n",
    "            if test_img.endswith('.jpg'):\n",
    "                src_test_img = os.path.join(test_images_path, test_img)\n",
    "                dest_test_img = os.path.join(output_path, 'test', 'images', test_img)\n",
    "                copy_file(src_test_img, dest_test_img)\n",
    "\n",
    "# Simpan metadata\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "metadata_df = metadata_df.sort_values(by=['split', 'country', 'filename'])\n",
    "metadata_df.to_csv(os.path.join(output_path, 'metadata.csv'), index=False)\n",
    "\n",
    "print(\"✅ Dataset berhasil digabung dan displit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Jumlah File per Split ===\n",
      "+-------+---------------+--------------------+\n",
      "| Split | Images (.jpg) | Annotations (.xml) |\n",
      "+-------+---------------+--------------------+\n",
      "| Train |     10734     |       10734        |\n",
      "|  Val  |     2814      |        2814        |\n",
      "| Test  |     9035      |         -          |\n",
      "+-------+---------------+--------------------+\n",
      "\n",
      "=== Distribusi Metadata per Split ===\n",
      "+-------+-------+\n",
      "| Split | Count |\n",
      "+-------+-------+\n",
      "| train | 10734 |\n",
      "|  val  | 2814  |\n",
      "+-------+-------+\n",
      "\n",
      "=== Distribusi Metadata per Negara ===\n",
      "+-----------------+-------+\n",
      "|     Country     | Count |\n",
      "+-----------------+-------+\n",
      "|  United_States  | 3907  |\n",
      "|      Japan      | 2797  |\n",
      "|     Norway      | 2518  |\n",
      "| China_MotorBike | 1419  |\n",
      "|      India      | 1109  |\n",
      "|   China_Drone   | 1034  |\n",
      "|      Czech      |  764  |\n",
      "+-----------------+-------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "root_path = \"D:\\\\Pothole Vision - AI Road Damage Detection\\\\dataset-mix\"\n",
    "\n",
    "def count_files(path, ext):\n",
    "    return len([f for f in os.listdir(path) if f.endswith(ext)]) if os.path.exists(path) else 0\n",
    "\n",
    "# Data jumlah file berdasarkan split\n",
    "data = {\n",
    "    'Split': ['Train', 'Val', 'Test'],\n",
    "    'Images (.jpg)': [\n",
    "        count_files(os.path.join(root_path, 'train', 'images'), '.jpg'),\n",
    "        count_files(os.path.join(root_path, 'val', 'images'), '.jpg'),\n",
    "        count_files(os.path.join(root_path, 'test', 'images'), '.jpg')\n",
    "    ],\n",
    "    'Annotations (.xml)': [\n",
    "        count_files(os.path.join(root_path, 'train', 'annotations'), '.xml'),\n",
    "        count_files(os.path.join(root_path, 'val', 'annotations'), '.xml'),\n",
    "        '-'  # Test tidak punya anotasi\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_split = pd.DataFrame(data)\n",
    "\n",
    "# Membaca metadata.csv\n",
    "metadata_path = os.path.join(root_path, 'metadata.csv')\n",
    "if os.path.exists(metadata_path):\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "    split_counts = metadata['split'].value_counts().reset_index()\n",
    "    split_counts.columns = ['Split', 'Count']\n",
    "\n",
    "    country_counts = metadata['country'].value_counts().reset_index()\n",
    "    country_counts.columns = ['Country', 'Count']\n",
    "\n",
    "    # Tampilkan tabel\n",
    "    print(\"=== Jumlah File per Split ===\")\n",
    "    print(tabulate(df_split, headers='keys', tablefmt='pretty', showindex=False))\n",
    "    print(\"\\n=== Distribusi Metadata per Split ===\")\n",
    "    print(tabulate(split_counts, headers='keys', tablefmt='pretty', showindex=False))\n",
    "    print(\"\\n=== Distribusi Metadata per Negara ===\")\n",
    "    print(tabulate(country_counts, headers='keys', tablefmt='pretty', showindex=False))\n",
    "else:\n",
    "    print(\"❌ File metadata.csv tidak ditemukan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.yaml')  # atau ganti ke 'yolov8s.yaml' jika mau versi sedikit lebih besar\n",
    "\n",
    "model.train(\n",
    "    data='rdd2022.yaml',\n",
    "    epochs=20,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name='yolov8_d00_only',\n",
    "    project='runs/train'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
