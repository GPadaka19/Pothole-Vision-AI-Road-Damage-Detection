{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cek jumlah beserta struktur datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China_Drone -> Train Images: 1034, Test Images: 0, Annotations: 1034\n",
      "China_MotorBike -> Train Images: 1419, Test Images: 500, Annotations: 1419\n",
      "Czech -> Train Images: 764, Test Images: 709, Annotations: 764\n",
      "India -> Train Images: 1109, Test Images: 1959, Annotations: 1109\n",
      "Japan -> Train Images: 2797, Test Images: 2627, Annotations: 2797\n",
      "Norway -> Train Images: 2518, Test Images: 2040, Annotations: 2518\n",
      "United_States -> Train Images: 3907, Test Images: 1200, Annotations: 3907\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_path = \"D:\\Pothole Vision - AI Road Damage Detection\\dataset\\RDD2022_all_countries\"\n",
    "def count_files(path):\n",
    "    return len([f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]) if os.path.exists(path) else 0\n",
    "\n",
    "summary = []\n",
    "\n",
    "for country in os.listdir(root_path):\n",
    "    country_path = os.path.join(root_path, country)\n",
    "    if os.path.isdir(country_path):\n",
    "        train_images = count_files(os.path.join(country_path, \"train\", \"images\"))\n",
    "        test_images = count_files(os.path.join(country_path, \"test\", \"images\"))\n",
    "        xml_files = count_files(os.path.join(country_path, \"train\", \"annotations\", \"xmls\"))\n",
    "        summary.append(f\"{country} -> Train Images: {train_images}, Test Images: {test_images}, Annotations: {xml_files}\")\n",
    "\n",
    "for line in summary:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "China_drone tidak punya folder test, abaikan?\n",
    "Selanjutnya Saya akan split datset dari Train & Test menjadi Train, Test, & Val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jika dataset DIGABUNG semua negara menjadi satu dataset besar:\n",
    "Keuntungan:\n",
    "\n",
    "Model akan lebih general karena belajar dari berbagai jenis jalan, cuaca, kamera.\n",
    "\n",
    "Bisa membantu jika nanti digunakan di Indonesia yang belum punya data.\n",
    "\n",
    "Jumlah data menjadi sangat besar (10.000++), sangat bagus untuk deep learning.\n",
    "\n",
    "Kekurangan:\n",
    "\n",
    "Bisa menyebabkan bias ke negara dengan data terbanyak (misalnya India, Japan).\n",
    "\n",
    "Anotasi antar negara mungkin memiliki inkonsistensi kecil (labeling style, noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rekomendasi untuk kasus ini:\n",
    "Karena kamu akan pakai untuk Indonesia, tapi belum punya data lokal, maka:\n",
    "\n",
    "Gabungkan semua negara ‚Üí latih model global, supaya kuat terhadap variasi.\n",
    "\n",
    "Simpan metadata negara asalnya ‚Üí bisa dipakai untuk evaluasi per negara.\n",
    "\n",
    "Nanti, jika ada data Indonesia, kamu bisa fine-tune model global ke data lokal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m file_path = os.path.join(annotation_folder, filename)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     tree = \u001b[43mET\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     root = tree.getroot()\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m root.findall(\u001b[33m'\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\xml\\etree\\ElementTree.py:1218\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(source, parser)\u001b[39m\n\u001b[32m   1209\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse XML document into element tree.\u001b[39;00m\n\u001b[32m   1210\u001b[39m \n\u001b[32m   1211\u001b[39m \u001b[33;03m*source* is a filename or file object containing XML data,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1215\u001b[39m \n\u001b[32m   1216\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1217\u001b[39m tree = ElementTree()\n\u001b[32m-> \u001b[39m\u001b[32m1218\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\xml\\etree\\ElementTree.py:569\u001b[39m, in \u001b[36mElementTree.parse\u001b[39m\u001b[34m(self, source, parser)\u001b[39m\n\u001b[32m    567\u001b[39m close_source = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(source, \u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m     source = \u001b[38;5;28mopen\u001b[39m(source, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    570\u001b[39m     close_source = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "root_path = \"dataset/RDD2022_all_countries\"\n",
    "\n",
    "country_folders = [\n",
    "    \"China_Drone\", \"China_MotorBike\", \"Czech\",\n",
    "    \"India\", \"Japan\", \"Norway\", \"United_States\"\n",
    "]\n",
    "\n",
    "all_labels = []\n",
    "\n",
    "for country in country_folders:\n",
    "    annotation_folder = os.path.join(root_path, country, \"train\", \"annotations\", \"xmls\")\n",
    "    if os.path.exists(annotation_folder):\n",
    "        for filename in os.listdir(annotation_folder):\n",
    "            if filename.endswith('.xml'):\n",
    "                file_path = os.path.join(annotation_folder, filename)\n",
    "                try:\n",
    "                    tree = ET.parse(file_path)\n",
    "                    root = tree.getroot()\n",
    "                    for obj in root.findall('object'):\n",
    "                        label = obj.find('name').text.strip()\n",
    "                        all_labels.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error parsing {file_path}: {e}\")\n",
    "\n",
    "label_counts = Counter(all_labels)\n",
    "\n",
    "print(\"Distribusi label di semua dataset mentah:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proses selesai.\n",
      "File XML dan gambar yang dihapus: 24837\n",
      "File XML yang dipertahankan: 13548\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "root_path = \"dataset/RDD2022_all_countries\"\n",
    "\n",
    "def clean_annotations(root_path, target_label='D00'):\n",
    "    removed_files = 0\n",
    "    kept_files = 0\n",
    "    for country in os.listdir(root_path):\n",
    "        country_path = os.path.join(root_path, country)\n",
    "        if not os.path.isdir(country_path):\n",
    "            continue\n",
    "\n",
    "        # Proses folder train annotations (ubah jika perlu val/test juga)\n",
    "        annotations_dir = os.path.join(country_path, 'train', 'annotations', 'xmls')\n",
    "        images_dir = os.path.join(country_path, 'train', 'images')\n",
    "\n",
    "        if not os.path.exists(annotations_dir):\n",
    "            print(f\"Folder anotasi tidak ditemukan: {annotations_dir}, dilewati.\")\n",
    "            continue\n",
    "\n",
    "        for xml_file in os.listdir(annotations_dir):\n",
    "            if not xml_file.endswith('.xml'):\n",
    "                continue\n",
    "\n",
    "            xml_path = os.path.join(annotations_dir, xml_file)\n",
    "            image_file = xml_file.replace('.xml', '.jpg')\n",
    "            image_path = os.path.join(images_dir, image_file)\n",
    "\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            # Cari objek yang labelnya bukan target_label, hapus mereka\n",
    "            objects = root.findall('object')\n",
    "            removed_objs = 0\n",
    "            for obj in objects:\n",
    "                label = obj.find('name').text.strip()\n",
    "                if label != target_label:\n",
    "                    root.remove(obj)\n",
    "                    removed_objs += 1\n",
    "\n",
    "            # Cek apakah setelah penghapusan masih ada objek\n",
    "            if len(root.findall('object')) == 0:\n",
    "                # Hapus XML dan gambarnya\n",
    "                os.remove(xml_path)\n",
    "                if os.path.exists(image_path):\n",
    "                    os.remove(image_path)\n",
    "                removed_files += 1\n",
    "            else:\n",
    "                # Simpan ulang XML yang sudah dibersihkan\n",
    "                tree.write(xml_path)\n",
    "                kept_files += 1\n",
    "\n",
    "    print(f\"Proses selesai.\")\n",
    "    print(f\"File XML dan gambar yang dihapus: {removed_files}\")\n",
    "    print(f\"File XML yang dipertahankan: {kept_files}\")\n",
    "\n",
    "clean_annotations(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset berhasil digabung dan displit.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Lokasi dataset dan output\n",
    "root_path = \"D:\\\\Pothole Vision - AI Road Damage Detection\\\\dataset\\\\RDD2022_all_countries\"\n",
    "output_path = \"D:\\\\Pothole Vision - AI Road Damage Detection\\\\dataset-mix\"\n",
    "train_val_split = 0.8  # 80% untuk train, 20% untuk val\n",
    "\n",
    "# Membuat direktori output\n",
    "os.makedirs(os.path.join(output_path, 'train', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, 'train', 'annotations'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, 'val', 'images'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, 'val', 'annotations'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_path, 'test', 'images'), exist_ok=True)\n",
    "\n",
    "# Metadata untuk csv\n",
    "metadata = []\n",
    "\n",
    "def copy_file(src_file, dest_file):\n",
    "    shutil.copy(src_file, dest_file)\n",
    "\n",
    "# Proses per negara\n",
    "for country in os.listdir(root_path):\n",
    "    country_path = os.path.join(root_path, country)\n",
    "    if not os.path.isdir(country_path):\n",
    "        continue\n",
    "\n",
    "    images_path = os.path.join(country_path, 'train', 'images')\n",
    "    annotations_path = os.path.join(country_path, 'train', 'annotations', 'xmls')\n",
    "\n",
    "    if not os.path.exists(images_path) or not os.path.exists(annotations_path):\n",
    "        continue\n",
    "\n",
    "    image_files = sorted([f for f in os.listdir(images_path) if f.endswith('.jpg')])\n",
    "    annotation_files = sorted([f for f in os.listdir(annotations_path) if f.endswith('.xml')])\n",
    "\n",
    "    # Pastikan hanya file yang cocok (image dan XML) yang digunakan\n",
    "    matched_files = []\n",
    "    for image in image_files:\n",
    "        basename = os.path.splitext(image)[0]\n",
    "        if f\"{basename}.xml\" in annotation_files:\n",
    "            matched_files.append((image, f\"{basename}.xml\"))\n",
    "\n",
    "    for image, annotation in matched_files:\n",
    "        src_image = os.path.join(images_path, image)\n",
    "        src_annotation = os.path.join(annotations_path, annotation)\n",
    "\n",
    "        split = 'train' if random.random() < train_val_split else 'val'\n",
    "        dest_image = os.path.join(output_path, split, 'images', image)\n",
    "        dest_annotation = os.path.join(output_path, split, 'annotations', annotation)\n",
    "\n",
    "        copy_file(src_image, dest_image)\n",
    "        copy_file(src_annotation, dest_annotation)\n",
    "\n",
    "        metadata.append({'filename': image, 'country': country, 'split': split})\n",
    "\n",
    "    # Test set (copy ke test/images tanpa anotasi)\n",
    "    test_images_path = os.path.join(country_path, 'test', 'images')\n",
    "    if os.path.exists(test_images_path):\n",
    "        for test_img in os.listdir(test_images_path):\n",
    "            if test_img.endswith('.jpg'):\n",
    "                src_test_img = os.path.join(test_images_path, test_img)\n",
    "                dest_test_img = os.path.join(output_path, 'test', 'images', test_img)\n",
    "                copy_file(src_test_img, dest_test_img)\n",
    "\n",
    "# Simpan metadata\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "metadata_df = metadata_df.sort_values(by=['split', 'country', 'filename'])\n",
    "metadata_df.to_csv(os.path.join(output_path, 'metadata.csv'), index=False)\n",
    "\n",
    "print(\"‚úÖ Dataset berhasil digabung dan displit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Jumlah File per Split ===\n",
      "+-------+---------------+--------------------+\n",
      "| Split | Images (.jpg) | Annotations (.xml) |\n",
      "+-------+---------------+--------------------+\n",
      "| Train |     10734     |       10734        |\n",
      "|  Val  |     2814      |        2814        |\n",
      "| Test  |     9035      |         -          |\n",
      "+-------+---------------+--------------------+\n",
      "\n",
      "=== Distribusi Metadata per Split ===\n",
      "+-------+-------+\n",
      "| Split | Count |\n",
      "+-------+-------+\n",
      "| train | 10734 |\n",
      "|  val  | 2814  |\n",
      "+-------+-------+\n",
      "\n",
      "=== Distribusi Metadata per Negara ===\n",
      "+-----------------+-------+\n",
      "|     Country     | Count |\n",
      "+-----------------+-------+\n",
      "|  United_States  | 3907  |\n",
      "|      Japan      | 2797  |\n",
      "|     Norway      | 2518  |\n",
      "| China_MotorBike | 1419  |\n",
      "|      India      | 1109  |\n",
      "|   China_Drone   | 1034  |\n",
      "|      Czech      |  764  |\n",
      "+-----------------+-------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "root_path = \"D:\\\\Pothole Vision - AI Road Damage Detection\\\\dataset-mix\"\n",
    "\n",
    "def count_files(path, ext):\n",
    "    return len([f for f in os.listdir(path) if f.endswith(ext)]) if os.path.exists(path) else 0\n",
    "\n",
    "# Data jumlah file berdasarkan split\n",
    "data = {\n",
    "    'Split': ['Train', 'Val', 'Test'],\n",
    "    'Images (.jpg)': [\n",
    "        count_files(os.path.join(root_path, 'train', 'images'), '.jpg'),\n",
    "        count_files(os.path.join(root_path, 'val', 'images'), '.jpg'),\n",
    "        count_files(os.path.join(root_path, 'test', 'images'), '.jpg')\n",
    "    ],\n",
    "    'Annotations (.xml)': [\n",
    "        count_files(os.path.join(root_path, 'train', 'annotations'), '.xml'),\n",
    "        count_files(os.path.join(root_path, 'val', 'annotations'), '.xml'),\n",
    "        '-'  # Test tidak punya anotasi\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_split = pd.DataFrame(data)\n",
    "\n",
    "# Membaca metadata.csv\n",
    "metadata_path = os.path.join(root_path, 'metadata.csv')\n",
    "if os.path.exists(metadata_path):\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "    split_counts = metadata['split'].value_counts().reset_index()\n",
    "    split_counts.columns = ['Split', 'Count']\n",
    "\n",
    "    country_counts = metadata['country'].value_counts().reset_index()\n",
    "    country_counts.columns = ['Country', 'Count']\n",
    "\n",
    "    # Tampilkan tabel\n",
    "    print(\"=== Jumlah File per Split ===\")\n",
    "    print(tabulate(df_split, headers='keys', tablefmt='pretty', showindex=False))\n",
    "    print(\"\\n=== Distribusi Metadata per Split ===\")\n",
    "    print(tabulate(split_counts, headers='keys', tablefmt='pretty', showindex=False))\n",
    "    print(\"\\n=== Distribusi Metadata per Negara ===\")\n",
    "    print(tabulate(country_counts, headers='keys', tablefmt='pretty', showindex=False))\n",
    "else:\n",
    "    print(\"‚ùå File metadata.csv tidak ditemukan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist Sebelum Training\n",
    "-----------------------------------------\n",
    "| Langkah                      | Status |\n",
    "| ---------------------------- | ------ |\n",
    "| Label hanya `D00`            | ‚úÖ     |\n",
    "| Dataset bersih & rapi        | ‚úÖ     |\n",
    "| Split 80:20                  | ‚úÖ     |\n",
    "| Metadata terdokumentasi      | ‚úÖ     |\n",
    "| Format dataset per algoritma | üîú     |\n",
    "| Training script siap pakai   | üîú     |\n",
    "| Evaluasi & logging per model | üîú     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Struktur direktori berhasil dibuat.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Path sumber dataset yang sudah dibersihkan dan displit\n",
    "SOURCE_DATASET = \"D:/Pothole Vision - AI Road Damage Detection/dataset-mix\"\n",
    "TARGET_ROOT = \"D:/Pothole Vision - AI Road Damage Detection/prepared-datasets\"\n",
    "\n",
    "# Pastikan target root dibuat\n",
    "os.makedirs(TARGET_ROOT, exist_ok=True)\n",
    "\n",
    "# Daftar algoritma\n",
    "algorithms = [\"yolov8\", \"ssd\", \"retinanet\", \"deformable_detr\", \"cornernet\"]\n",
    "\n",
    "# Buat direktori dasar untuk masing-masing algoritma\n",
    "for algo in algorithms:\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        os.makedirs(os.path.join(TARGET_ROOT, algo, split, \"images\"), exist_ok=True)\n",
    "        if split != \"test\":  # hanya train dan val yang memiliki annotation\n",
    "            os.makedirs(os.path.join(TARGET_ROOT, algo, split, \"annotations\"), exist_ok=True)\n",
    "print(\"‚úÖ Struktur direktori berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Menyalin dataset untuk yolov8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10734/10734 [00:53<00:00, 200.22it/s]\n",
      "train annotations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10734/10734 [01:02<00:00, 172.84it/s]\n",
      "val images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2814/2814 [00:15<00:00, 180.71it/s]\n",
      "val annotations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2814/2814 [00:16<00:00, 173.68it/s]\n",
      "test images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9035/9035 [00:59<00:00, 152.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Menyalin dataset untuk ssd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10734/10734 [01:20<00:00, 133.63it/s]\n",
      "train annotations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10734/10734 [00:58<00:00, 184.77it/s]\n",
      "val images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2814/2814 [00:19<00:00, 147.01it/s]\n",
      "val annotations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2814/2814 [00:16<00:00, 171.44it/s]\n",
      "test images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9035/9035 [00:57<00:00, 157.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Menyalin dataset untuk retinanet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10734/10734 [01:04<00:00, 165.51it/s]\n",
      "train annotations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10734/10734 [00:55<00:00, 192.52it/s]\n",
      "val images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2814/2814 [00:15<00:00, 178.92it/s]\n",
      "val annotations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2814/2814 [00:16<00:00, 169.34it/s]\n",
      "test images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9035/9035 [00:56<00:00, 158.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Menyalin dataset untuk deformable_detr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10734/10734 [01:13<00:00, 146.12it/s]\n",
      "train annotations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10734/10734 [00:52<00:00, 204.41it/s]\n",
      "val images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2814/2814 [00:14<00:00, 192.66it/s]\n",
      "val annotations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2814/2814 [00:13<00:00, 202.94it/s]\n",
      "test images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9035/9035 [00:54<00:00, 166.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Menyalin dataset untuk cornernet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10734/10734 [01:06<00:00, 160.90it/s]\n",
      "train annotations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10734/10734 [01:08<00:00, 157.12it/s]\n",
      "val images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2814/2814 [00:18<00:00, 150.87it/s]\n",
      "val annotations: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2814/2814 [00:17<00:00, 160.65it/s]\n",
      "test images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9035/9035 [00:58<00:00, 153.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Semua data berhasil diduplikasi ke masing-masing algoritma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for algo in algorithms:\n",
    "    print(f\"\\nüìÅ Menyalin dataset untuk {algo}...\")\n",
    "    for split in splits:\n",
    "        src_img_dir = os.path.join(SOURCE_DATASET, split, \"images\")\n",
    "        dest_img_dir = os.path.join(TARGET_ROOT, algo, split, \"images\")\n",
    "        for f in tqdm(os.listdir(src_img_dir), desc=f\"{split} images\"):\n",
    "            if f.endswith(\".jpg\"):\n",
    "                shutil.copy(os.path.join(src_img_dir, f), os.path.join(dest_img_dir, f))\n",
    "\n",
    "        if split != \"test\":\n",
    "            src_ann_dir = os.path.join(SOURCE_DATASET, split, \"annotations\")\n",
    "            dest_ann_dir = os.path.join(TARGET_ROOT, algo, split, \"annotations\")\n",
    "            for f in tqdm(os.listdir(src_ann_dir), desc=f\"{split} annotations\"):\n",
    "                if f.endswith(\".xml\"):\n",
    "                    shutil.copy(os.path.join(src_ann_dir, f), os.path.join(dest_ann_dir, f))\n",
    "print(\"\\n‚úÖ Semua data berhasil diduplikasi ke masing-masing algoritma.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahap selanjutnya: konversi annotation ke format:\n",
    "- YOLOv8      ‚Üí .txt (YOLO format)\n",
    "- SSD         ‚Üí TFRecord / COCO JSON\n",
    "- RetinaNet   ‚Üí COCO JSON\n",
    "- Deformable DETR ‚Üí COCO JSON\n",
    "- CornerNet   ‚Üí COCO JSON (keypoint-style bounding box if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+-------------------+------------+-----------------+-------------+------------------+\n",
      "|    Algorithm    | train_images | train_annotations | val_images | val_annotations | test_images | test_annotations |\n",
      "+-----------------+--------------+-------------------+------------+-----------------+-------------+------------------+\n",
      "|     yolov8      |    10734     |       10734       |    2814    |      2814       |    9035     |        -         |\n",
      "|       ssd       |    10734     |       10734       |    2814    |      2814       |    9035     |        -         |\n",
      "|    retinanet    |    10734     |       10734       |    2814    |      2814       |    9035     |        -         |\n",
      "| deformable_detr |    10734     |       10734       |    2814    |      2814       |    9035     |        -         |\n",
      "|    cornernet    |    10734     |       10734       |    2814    |      2814       |    9035     |        -         |\n",
      "+-----------------+--------------+-------------------+------------+-----------------+-------------+------------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "summary = []\n",
    "for algo in algorithms:\n",
    "    row = {\"Algorithm\": algo}\n",
    "    for split in splits:\n",
    "        img_dir = os.path.join(TARGET_ROOT, algo, split, \"images\")\n",
    "        ann_dir = os.path.join(TARGET_ROOT, algo, split, \"annotations\") if split != \"test\" else \"-\"\n",
    "        row[f\"{split}_images\"] = len(os.listdir(img_dir))\n",
    "        row[f\"{split}_annotations\"] = len(os.listdir(ann_dir)) if ann_dir != \"-\" else \"-\"\n",
    "    summary.append(row)\n",
    "\n",
    "df = pd.DataFrame(summary)\n",
    "print(tabulate(df, headers='keys', tablefmt='pretty', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konversi anotasi XML (Pascal VOC) ke format masing-masing algoritma\n",
    "# Output disimpan di folder prepared-datasets/{algo}/\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset sumber\n",
    "SOURCE_IMAGES_DIR = \"dataset-mix\"\n",
    "SOURCE_ANN_DIR = {\n",
    "    'train': os.path.join(SOURCE_IMAGES_DIR, 'train', 'annotations'),\n",
    "    'val': os.path.join(SOURCE_IMAGES_DIR, 'val', 'annotations'),\n",
    "}\n",
    "\n",
    "# Target direktori per algoritma\n",
    "ALGORITHMS = ['yolov8', 'ssd', 'retinanet', 'deformable_detr', 'cornernet']\n",
    "PREPARED_ROOT = \"prepared-datasets\"\n",
    "\n",
    "# Pastikan direktori target tersedia\n",
    "def prepare_dirs():\n",
    "    for algo in ALGORITHMS:\n",
    "        for split in ['train', 'val']:\n",
    "            os.makedirs(os.path.join(PREPARED_ROOT, algo, split, 'images'), exist_ok=True)\n",
    "            os.makedirs(os.path.join(PREPARED_ROOT, algo, split, 'annotations'), exist_ok=True)\n",
    "\n",
    "# Konversi ke format YOLOv8\n",
    "# Hanya menyimpan kelas D00 dengan index 0\n",
    "def convert_to_yolo():\n",
    "    for split in ['train', 'val']:\n",
    "        image_dir = os.path.join(SOURCE_IMAGES_DIR, split, 'images')\n",
    "        ann_dir = SOURCE_ANN_DIR[split]\n",
    "        target_img_dir = os.path.join(PREPARED_ROOT, 'yolov8', split, 'images')\n",
    "        target_label_dir = os.path.join(PREPARED_ROOT, 'yolov8', split, 'annotations')\n",
    "\n",
    "        for file in tqdm(os.listdir(ann_dir), desc=f\"[YOLO] Converting {split}\"):\n",
    "            if not file.endswith(\".xml\"): continue\n",
    "            xml_path = os.path.join(ann_dir, file)\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "            image_filename = root.find('filename').text\n",
    "            image_path = os.path.join(image_dir, image_filename)\n",
    "            out_image_path = os.path.join(target_img_dir, image_filename)\n",
    "\n",
    "            # Symlink image\n",
    "            if not os.path.exists(out_image_path):\n",
    "                os.symlink(os.path.abspath(image_path), out_image_path)\n",
    "\n",
    "            size = root.find(\"size\")\n",
    "            w, h = int(size.find(\"width\").text), int(size.find(\"height\").text)\n",
    "            yolo_lines = []\n",
    "\n",
    "            for obj in root.findall(\"object\"):\n",
    "                name = obj.find(\"name\").text.strip()\n",
    "                if name != \"D00\":\n",
    "                    continue  # Skip non-D00\n",
    "\n",
    "                bndbox = obj.find(\"bndbox\")\n",
    "                xmin = int(float(bndbox.find(\"xmin\").text))\n",
    "                ymin = int(float(bndbox.find(\"ymin\").text))\n",
    "                xmax = int(float(bndbox.find(\"xmax\").text))\n",
    "                ymax = int(float(bndbox.find(\"ymax\").text))\n",
    "\n",
    "                # Convert to YOLO format\n",
    "                x_center = ((xmin + xmax) / 2) / w\n",
    "                y_center = ((ymin + ymax) / 2) / h\n",
    "                bw = (xmax - xmin) / w\n",
    "                bh = (ymax - ymin) / h\n",
    "                yolo_lines.append(f\"0 {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}\")\n",
    "\n",
    "            # Simpan hasil label YOLO\n",
    "            txt_path = os.path.join(target_label_dir, file.replace(\".xml\", \".txt\"))\n",
    "            with open(txt_path, \"w\") as f:\n",
    "                f.write(\"\\n\".join(yolo_lines))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    prepare_dirs()\n",
    "    convert_to_yolo()\n",
    "    print(\"‚úÖ Konversi YOLOv8 selesai. Untuk algoritma lain, dilanjutkan dengan modul terpisah.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n.yaml')  # atau ganti ke 'yolov8s.yaml' jika mau versi sedikit lebih besar\n",
    "\n",
    "model.train(\n",
    "    data='rdd2022.yaml',\n",
    "    epochs=20,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name='yolov8_d00_only',\n",
    "    project='runs/train'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
